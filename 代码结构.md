nano-vllm/
├── example.py                # [入口] 用户使用示例，展示如何调用 API
├── bench.py                  # [测试] 性能基准测试脚本
├── nanovllm/                 # [核心包]
│   ├── llm.py                # [接口] 用户门面类 (Facade)，统一入口
│   ├── sampling_params.py    # [数据] 采样参数定义 (温度, max_tokens 等)
│   ├── config.py             # [配置] 模型与系统配置类
│   │
│   ├── engine/               # [控制层] 系统的“大脑” (最为核心)
│   │   ├── llm_engine.py     # ★ [总控] 协调调度器与模型执行者，包含主循环 step()
│   │   ├── scheduler.py      # ★ [调度] 连续批处理 (Continuous Batching) 逻辑
│   │   ├── block_manager.py  # ★ [显存] PagedAttention 显存管理 (KV Block 分配)
│   │   ├── sequence.py       # [状态] 请求序列的状态管理 (Token IDs, Block Table)
│   │   └── model_runner.py   # ★ [执行] GPU 交互、KV Cache 预分配、CUDA Graph 录制
│   │
│   ├── models/               # [模型层] 模型架构定义
│   │   └── qwen3.py          # [具体模型] Qwen3 的实现 (支持 Tensor Parallel)
│   │
│   ├── layers/               # [算子层] 具体的神经网络层实现
│   │   ├── attention.py      # ★ [算子] PagedAttention 核心实现 & FlashAttention 调用
│   │   ├── linear.py         # ★ [算子] 张量并行 (TP) 线性层 (Row/Column Parallel)
│   │   ├── embed_head.py     # [算子] 并行化的 Embedding 和 LM Head
│   │   ├── rotary_embedding.py # [算子] RoPE 位置编码
│   │   ├── sampler.py        # [算子] 采样器 (Logits -> Token)
│   │   ├── activation.py     # [算子] 激活函数
│   │   └── layernorm.py      # [算子] RMSNorm
│   │
│   └── utils/                # [工具层]
│       ├── context.py        # [上下文] 全局上下文，用于跨层传递 metadata
│       └── loader.py         # [加载] 模型权重加载逻辑

2. 模块详解与核心代码 (Detailed Breakdown)
我们将整个系统分为四个逻辑层级。对于你研究“大模型推理”和“分布式计算”的目标，打 ★ 的部分是重中之重。

第一层：用户接口 (User Interface)
这层决定了用户怎么输入，以及系统怎么输出。

llm.py:
作用: 包装了底层的 LLMEngine，提供了类似 vLLM 的简单 API（如 generate）。
核心逻辑: 主要是参数传递，没有复杂的计算逻辑。

sampling_params.py:
作用: 定义采样相关的参数结构体。

# 第二层：引擎核心 (The Engine - Control Plane)
这是整个系统的灵魂，负责资源调度和流程控制。

★ nanovllm/engine/llm_engine.py:
作用: 指挥官。初始化 Scheduler 和 ModelRunner。
核心代码: step() 方法。它在一个循环里完成：调度 (schedule) -> 执行 (run) -> 后处理 (postprocess) 的全过程。

★ nanovllm/engine/scheduler.py:
作用: 调度器。决定下一帧计算要处理哪些请求。
核心代码: schedule() 方法。它实现了 Continuous Batching，优先处理 Running 队列（Decode 阶段），如果显存够再从 Waiting 队列加入新请求（Prefill 阶段）。

★ nanovllm/engine/block_manager.py:
作用: 显存管家。实现了 PagedAttention 的内存管理逻辑。
核心代码: allocate() 和 Block 类。它将逻辑上的 Token 映射到物理的显存块 ID 上，类似于操作系统的页表管理。

# 第三层：执行与硬件交互 (The Runner - Data Plane)
这层负责指挥 GPU 干活，是你研究分布式最相关的部分。
 
★ nanovllm/engine/model_runner.py:
作用: GPU 代理人。负责实际的模型前向传播。
核心代码:
allocate_kv_cache(): 预先在 GPU 上分配巨大的 KV Cache 张量。
prepare_prefill / prepare_decode: 准备输入数据和 slot_mapping（告诉 Attention 层每个 Token 的 KV 存在哪里）。
capture_cudagraph(): 使用 CUDA Graph 优化小 Batch 的执行效率。
init_process_group: 这里涉及分布式通信组的初始化。

# 第四层：模型架构与算子 (Model Architecture & Layers)
这层定义了具体的计算逻辑。

★ nanovllm/layers/attention.py:
作用: 核心算子。
核心代码: forward()。它根据全局 Context 判断是 Prefill 还是 Decode，并调用 flash_attn 或自定义的 PagedAttention 逻辑。store_kvcache 内核负责将计算出的 KV 写入显存块。

★ nanovllm/layers/linear.py:
作用: 分布式计算的基础。
核心代码: ColumnParallelLinear 和 RowParallelLinear。
关注点: 这里的 forward 函数中包含了 dist.all_reduce(y)。你研究的“计算与通信重叠”主要就是针对这里的 all_reduce 操作进行优化。

3. 数据流向简述 (Data Flow)
为了帮你串联起来，一个请求的处理流程如下：
用户调用: llm.generate("Hello") -> 创建 Sequence 对象。
入队: 请求被放入 Scheduler 的 waiting 队列。
调度 (Engine.step):
Scheduler 检查显存（咨询 BlockManager），发现有空位。
BlockManager 分配物理 Block ID。
请求从 waiting 移入 running。

准备 (ModelRunner):
ModelRunner 拿到请求，生成 input _ids 和关键的 slot_mapping（逻辑块到物理块的映射）。

执行 (Model):
模型层层计算。

遇到 Attention 层时，根据 slot_mapping 将 KV 写入预分配的显存池。
遇到 Linear 层时，进行矩阵乘法（可能涉及多卡通信）。

输出: 采样出下一个 Token，追加到 Sequence 中。循环回到第 3 步，直到生成结束。

建议你先从 nanovllm/engine/llm_engine.py 的 step 函数看起，它是整个流程的驱动器